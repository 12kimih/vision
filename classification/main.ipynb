{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3950cdb550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "from models.efficientnet import efficientnet_b0, efficientnetv2_s\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available datasets: 'cifar10', 'cifar100', 'mnist', 'emnist'\n",
    "_dataset = 'emnist'\n",
    "\n",
    "if _dataset == 'cifar10':\n",
    "    dataset = datasets.CIFAR10\n",
    "elif _dataset == 'cifar100':\n",
    "    dataset = datasets.CIFAR100\n",
    "elif _dataset == 'mnist':\n",
    "    dataset = datasets.MNIST\n",
    "elif _dataset == 'emnist':\n",
    "    dataset = partial(datasets.EMNIST, split='letters')\n",
    "else:\n",
    "    raise ValueError('dataset only supports {cifar10|cifar100|mnist|emnist}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w g p o\n",
      "tensor([0.1722], dtype=torch.float64)\n",
      "tensor([0.1095], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_workers = 8\n",
    "\n",
    "trainset = dataset(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "classes = trainset.classes\n",
    "num_classes = len(classes)\n",
    "num_images = len(trainset)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "size = images.size(dim=2)\n",
    "channels = images.size(dim=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(torch.permute(make_grid(images[:4]), dims=(1, 2, 0)).numpy())\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "print(' '.join(f'{classes[labels[j]]}' for j in range(4)))\n",
    "\n",
    "count = torch.zeros(channels, dtype=torch.int64)\n",
    "sum = torch.zeros(channels, dtype=torch.float64)\n",
    "ss = torch.zeros(channels, dtype=torch.float64)\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    count += images.size(0) * images.size(2) * images.size(3)\n",
    "    sum += torch.sum(images, dim=(0, 2, 3))\n",
    "    ss += torch.sum(torch.square(images), dim=(0, 2, 3))\n",
    "\n",
    "mean = torch.div(sum, count)\n",
    "std = torch.sub(torch.div(ss, count), torch.square(mean))\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose batch size, number of workers. Build dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "trainset = dataset(root='./data', train=True, transform=transform, download=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "testset = dataset(root='./data', train=False, transform=transform, download=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose device, model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layers): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (fc): Linear(in_features=512, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_device = 'cuda'\n",
    "device = torch.device(_device)\n",
    "\n",
    "model = resnet18(size=size, in_channels=channels, num_classes=num_classes, dropout=0.2)\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose epochs, steps, lr, weight decay, optimizer, scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "steps = 100\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 2.5e-4\n",
    "T_0 = 1\n",
    "T_mult = 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load:\n",
    "    state = torch.load('./model.pt')\n",
    "\n",
    "    model.load_state_dict(state['model'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    scheduler.load_state_dict(state['scheduler'])\n",
    "\n",
    "    load_epoch = state['epoch']\n",
    "    cycle = state['cycle']\n",
    "    T_i = state['T_i']\n",
    "    best_accuracy = state['best_accuracy']\n",
    "else:\n",
    "    load_epoch = 1\n",
    "    cycle = 0\n",
    "    T_i = T_0\n",
    "    best_accuracy = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model. Redirect stdout to a log file. Save the best model at every cycle of lr scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ik/anaconda3/envs/vision/lib/python3.11/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "stdout = sys.stdout\n",
    "f = open('model.log', 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "T_cur = 0\n",
    "for epoch in range(load_epoch - 1, epochs):\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_count = 0\n",
    "    for step, (images, labels) in enumerate(trainloader):\n",
    "        model.train()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_correct += torch.sum(torch.argmax(outputs, dim=-1) == labels).item()\n",
    "        running_count += images.size(dim=0)\n",
    "\n",
    "        if (step + 1) % steps == 0:\n",
    "            print(f'[Epoch {epoch + 1:02d}] [Step {step + 1:04d}] Loss: {running_loss / (step + 1):.4f}, Accuracy: {running_correct / running_count:.4f}')\n",
    "\n",
    "        T_cur += 1\n",
    "\n",
    "        if T_cur == T_i:\n",
    "            cycle += 1\n",
    "            T_i *= T_mult\n",
    "            T_cur = 0\n",
    "\n",
    "            correct = 0\n",
    "            count = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in testloader:\n",
    "                    model.eval()\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    correct += torch.sum(torch.argmax(outputs, dim=-1) == labels).item()\n",
    "                    count += images.size(dim=0)\n",
    "\n",
    "            accuracy = correct / count\n",
    "            print(f'[Cycle {cycle:02d}] Accuracy: {accuracy:.4f}, Best accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                state = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'epoch': epoch + 1,\n",
    "                    'cycle': cycle,\n",
    "                    'T_i': T_i,\n",
    "                    'best_accuracy': best_accuracy,\n",
    "                }\n",
    "\n",
    "                torch.save(state, 'model.pt')\n",
    "                best_accuracy = accuracy\n",
    "                print(f'New best accuracy')\n",
    "\n",
    "sys.stdout = stdout\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ik/anaconda3/envs/vision/lib/python3.11/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8784\n"
     ]
    }
   ],
   "source": [
    "state = torch.load('./model.pt')\n",
    "\n",
    "model.load_state_dict(state['model'])\n",
    "\n",
    "correct = 0\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        model.eval()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        correct += torch.sum(torch.argmax(outputs, dim=-1) == labels).item()\n",
    "        count += images.size(dim=0)\n",
    "\n",
    "accuracy = correct / count\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
